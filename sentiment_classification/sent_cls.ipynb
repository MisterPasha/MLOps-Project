{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87465d7c",
   "metadata": {},
   "source": [
    "***Load Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f519d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "data = pd.read_csv('https://github.com/laxmimerit/All-CSV-ML-Data-Files-Download/raw/refs/heads/master/IMDB-Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08446aad",
   "metadata": {},
   "source": [
    "***Split Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1b79e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a59179",
   "metadata": {},
   "source": [
    "***Introduce numerical labels***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87b323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40ae2d2a8b7492fa659569bd75ae7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef0822d53f546499c04b59c24f0ed54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label2id = {'positive': 1, 'negative': 0}\n",
    "id2label = {1: 'positive', 0: 'negative'}\n",
    "\n",
    "dataset = dataset.map(lambda x: {'label': label2id[x['sentiment']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508373e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"This is by far THE WORST movie i have ever watched. I've seen some pretty awful movies in my time but this ones takes the cake, no, wait, i mean the the whole damn bakery. It is so bad that i believe a word to describe the way you will feel after watching this atrocity has yet to be created. Please just do yourself a favor, if you ever get the urge to watch this and watch thirty minutes of that annoying purple dinosaur Barney, then multiply that thirty times fold and you would still only get a small fraction of the horror you would be in store for. In summation, i guess you really can call it a horror movie, but only if you're willing to be scared senseless by the worst acting in the business and utterly pointless story.<br /><br />Real Rating, -10 Disgusting\",\n",
       " 'sentiment': 'negative',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea84968",
   "metadata": {},
   "source": [
    "***Tokenise Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad396b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1fe48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_ckpt, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c0b46bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49db130da3704310943238a1091dc3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247b9efe615b4092aa2cd0dea468f4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokeniser(dataset['train'][0]['review'])\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    temp = tokeniser(batch['review'], padding=True, truncation=True, max_length=300)\n",
    "    return temp\n",
    "\n",
    "tokenised_datasets = dataset.map(tokenize_function, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd902d",
   "metadata": {},
   "source": [
    "***Model Evaluation Function*** (From HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfd1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb8a72",
   "metadata": {},
   "source": [
    "***Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d10f5898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d797d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/pavel/Projects/mlops/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='train_tinybert_imdb',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenised_datasets['train'],\n",
    "    eval_dataset=tokenised_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokeniser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b410618a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3282' max='3282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3282/3282 1:41:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.340336</td>\n",
       "      <td>0.851467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.296662</td>\n",
       "      <td>0.877733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.293437</td>\n",
       "      <td>0.881200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3282, training_loss=0.31383063660969757, metrics={'train_runtime': 6114.6209, 'train_samples_per_second': 17.172, 'train_steps_per_second': 0.537, 'total_flos': 885410612150400.0, 'train_loss': 0.31383063660969757, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c24b603e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='469' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [469/469 06:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2934366464614868,\n",
       " 'eval_accuracy': 0.8812,\n",
       " 'eval_runtime': 387.5164,\n",
       " 'eval_samples_per_second': 38.708,\n",
       " 'eval_steps_per_second': 1.21,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223adea8",
   "metadata": {},
   "source": [
    "***Model Save and Load for Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8682ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('tinybert_imdb_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c0610d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"The movie was fantastic! I really loved it.\",\n",
    "        \"I hated the film. It was the worst I've ever seen.\",\n",
    "        \"It was an average movie. Not bad, but not great either.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cec279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.990028440952301},\n",
       " {'label': 'negative', 'score': 0.9872934818267822},\n",
       " {'label': 'negative', 'score': 0.8724518418312073}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model='tinybert_imdb_model', tokenizer=tokeniser, device=device)\n",
    "\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe410e",
   "metadata": {},
   "source": [
    "***Upload Model to AWS S3***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b895ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'tinybert-imdb-model-bucket-7429' already exists\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = \"tinybert-imdb-model-bucket-7429\"\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    resp = s3.list_buckets()\n",
    "    buckets = [bucket['Name'] for bucket in resp['Buckets']]\n",
    "    if bucket_name in buckets:\n",
    "        print(f\"Bucket '{bucket_name}' already exists\")\n",
    "        return\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    print(f\"Bucket '{bucket_name}' created\")\n",
    "\n",
    "create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f2a5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 'tinybert_imdb_model/config.json' to 'models/tinybert_imdb_model/config.json'\n",
      "Uploaded 'tinybert_imdb_model/model.safetensors' to 'models/tinybert_imdb_model/model.safetensors'\n",
      "Uploaded 'tinybert_imdb_model/special_tokens_map.json' to 'models/tinybert_imdb_model/special_tokens_map.json'\n",
      "Uploaded 'tinybert_imdb_model/tokenizer.json' to 'models/tinybert_imdb_model/tokenizer.json'\n",
      "Uploaded 'tinybert_imdb_model/tokenizer_config.json' to 'models/tinybert_imdb_model/tokenizer_config.json'\n",
      "Uploaded 'tinybert_imdb_model/training_args.bin' to 'models/tinybert_imdb_model/training_args.bin'\n",
      "Uploaded 'tinybert_imdb_model/vocab.txt' to 'models/tinybert_imdb_model/vocab.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def upload_directory(directory_path, s3_prefix, bucket_name=bucket_name):\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file).replace('\\\\', '/')\n",
    "            relpath = os.path.relpath(file_path, directory_path)\n",
    "            s3_key = os.path.join(s3_prefix, relpath).replace('\\\\', '/')\n",
    "            s3.upload_file(file_path, bucket_name, s3_key)\n",
    "            print(f\"Uploaded '{file_path}' to '{s3_key}'\")\n",
    "\n",
    "upload_directory('tinybert_imdb_model', 'models/tinybert_imdb_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
